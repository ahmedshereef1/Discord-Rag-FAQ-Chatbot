{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b935bc",
   "metadata": {},
   "source": [
    "# Data Scientist Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d991b",
   "metadata": {},
   "source": [
    "In this notebook, I will discuss the concept of RAG, defining how the bot understands questions, retrieves information, and how the LLM ultimately uses that information to generate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058f9f5",
   "metadata": {},
   "source": [
    "<img src=\"../Data//images/RAG Pipeline.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e27896",
   "metadata": {},
   "source": [
    "## Phase 1: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6cfde",
   "metadata": {},
   "source": [
    "It's essential to have a knowledge of the core components of RAG :\n",
    "\n",
    "1 - **Chunking documents**  \n",
    "2 - **Embedding**  \n",
    "3 - **Ingestion**  \n",
    "4 - **Vector Search**  \n",
    "5 - **(Retrieval + Generation)**  \n",
    "6 - **Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bf120",
   "metadata": {},
   "source": [
    "## HOW DO RAGs WORK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a20407",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"../Data/images/RAG Work.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "In the first step, there's an encoder that converts your raw text and documents into mathematical form, so the computer can understand them. So, all the words, sentences, or entire documents that make up your external database are converted into \"vectors.\" All these vectors (in the form of vector embeddings) will now be stored in a vector database. Note that this is a great way of capturing the semantics of different words, their relationship to other words, and what topics these words represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded85b4",
   "metadata": {},
   "source": [
    "### 1- Chunking documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd6ca3",
   "metadata": {},
   "source": [
    "Breaking large documents into smaller chunks so that AI models can better find, understand, and use the information when answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55c2db5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0941665",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Data/MlQuestions_Final_Comprehensive.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "abd9e19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6a45b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: Machine Learning Interview \n",
      "Questions & Answers  \n",
      "Disclaimer \n",
      "Machine Learning is an important concept when it comes to Data Science Interviews. \n",
      "Prepare for your Machine Learning Interviews with these most asked interview questions. \n",
      "Q. 1: Explain Bias-Variance Tradeoff. \n",
      "Ans: The bias-variance tradeoff represents the balance between the model's ability to \n",
      "generalize across different datasets (bias) and its sensitivity to small fluctuations in the \n",
      "training set (variance). A high-bias model is too simple and underfits the data, missing the \n",
      "underlying trend. A high-variance model is too complex, overfitting the data and capturing \n",
      "noise as if it were a real pattern. The goal is to find a sweet spot that minimizes the total \n",
      "error. \n",
      "Q. 2: How does Gradient Descent Work? \n",
      "Ans: Gradient Descent is an optimization algorithm used to minimize some function by \n",
      "iteratively moving in the direction of the steepest descent as defined by the negative of the \n",
      "gradient. In machine learning, it's used to find the parameters of a model that minimize the \n",
      "cost function. The learning rate determines the size of the steps taken to reach the \n",
      "minimum. \n",
      "Q. 3: What is Regularization? Give Examples. \n",
      "Ans: Regularization is a technique used to prevent overfitting by adding a penalty on the \n",
      "size of the coefficients. The penalty term discourages complex models and thus reduces \n",
      "variance without substantially increasing bias. Examples include L1 regularization (Lasso), \n",
      "which adds the absolute value of the magnitude of coefficients as penalty, and L2 \n",
      "regularization (Ridge), which adds the square of the magnitude of coefficients. \n",
      "Q. 4: Explain the Difference between Bagging and Boosting. \n",
      "Ans: Both Bagging and Boosting are ensemble techniques to improve model predictions, but \n",
      "they work differently. \n",
      " \n",
      "Comparison Table: \n",
      "1. Bagging: The simplest way of combining predictions that belong to the same type. \n",
      "Boosting: A way of combining predictions that belong to the different types.\n"
     ]
    }
   ],
   "source": [
    "page_one = pages[0]\n",
    "print(page_one.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29a3b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Microsoft® Word LTSC',\n",
       " 'creator': 'Microsoft® Word LTSC',\n",
       " 'creationdate': '2026-01-22T10:57:39+02:00',\n",
       " 'author': 'python-docx',\n",
       " 'moddate': '2026-01-22T10:57:39+02:00',\n",
       " 'source': '../Data/MlQuestions_Final_Comprehensive.pdf',\n",
       " 'total_pages': 8,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_one.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d0913",
   "metadata": {},
   "source": [
    "### 1. Why do we need Document Splitting or Chunking?\n",
    "\n",
    "In LangChain, document splitting (or text chunking) is an essential preprocessing step before feeding large documents into language models or vector databases.\n",
    "\n",
    "LLMs (Large Language Models) like GPT can only handle a limited number of tokens per request.\n",
    "So, we split long documents into smaller, manageable chunks — allowing:\n",
    "\n",
    "* Efficient retrieval\n",
    "* Better embeddings generation\n",
    "* Faster and more accurate responses during question answering or summarization tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "93b5d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1500 \n",
    "chunk_overlap = 200\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee25170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 15\n",
      "First chunk preview: Part 1: Machine Learning Interview \n",
      "Questions & Answers  \n",
      "Disclaimer \n",
      "Machine Learning is an important concept when it comes to Data Science Interviews. \n",
      "Prepare for your Machine Learning Interviews w\n"
     ]
    }
   ],
   "source": [
    "chunks = r_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"First chunk preview: {chunks[0].page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c80852",
   "metadata": {},
   "source": [
    "## 2 - Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ecbff",
   "metadata": {},
   "source": [
    "### What are Text Embeddings?\n",
    "\n",
    "Text embeddings are numerical representations of text — words, sentences, or even entire documents — that capture their meaning in a way that computers can understand and compare.\n",
    "\n",
    "Embeddings are the foundation for many NLP (Natural Language Processing) tasks:\n",
    "\n",
    " * Semantic search: Finding documents similar in meaning, not just keyword match.\n",
    "\n",
    "  * Chatbots / Retrieval-Augmented Generation (RAG): Retrieving relevant context from a database to answer questions.\n",
    "\n",
    " * Clustering: Grouping similar texts (reviews or news articles).\n",
    "\n",
    " * Recommendation systems: Suggesting similar content.\n",
    "\n",
    " * Sentiment or topic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4d1c7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key found and start with: sk-pr\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "generation_model_id = os.getenv(\"GENERATION_MODEL_ID\")\n",
    "embedded_model_id = os.getenv(\"EMBEDDING_MODEL_ID\")\n",
    "embeddid_model_size = os.getenv(\"EMBEDDING_MODEL_SIZE\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI key found and start with: {openai_api_key[:5]}\")\n",
    "else:\n",
    "    print(f\"Key not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c0f6e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 15 chunks...\n",
      "Created 15 embeddings\n",
      "Embedding dimension: 1024\n",
      "First chunk: Part 1: Machine Learning Interview \n",
      "Questions & Answers  \n",
      "Disclaimer \n",
      "Machine Learning is an importa...\n",
      "First embedding (first 5 values): [-0.0126571655, -0.009361267, -0.09033203, 0.046203613, -0.049041748]\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Initialize Cohere embeddings\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=embedded_model_id,\n",
    "    cohere_api_key=cohere_api_key\n",
    ")\n",
    "\n",
    "# Extract text content from Document objects\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# Test the embeddings\n",
    "print(f\"Embedding {len(chunk_texts)} chunks...\")\n",
    "chunk_embeddings = embeddings.embed_documents(chunk_texts)\n",
    "print(f\"Created {len(chunk_embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(chunk_embeddings[0])}\")\n",
    "print(f\"First chunk: {chunk_texts[0][:100]}...\")\n",
    "print(f\"First embedding (first 5 values): {chunk_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4bb5a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 15 chunks...\n",
      "Created 15 embeddings\n",
      "Embedding dimension: 384\n",
      "First chunk: Part 1: Machine Learning Interview \n",
      "Questions & Answers  \n",
      "Disclaimer \n",
      "Machine Learning is an importa...\n",
      "First embedding (first 5 values): [-0.05354616791009903, 0.061014801263809204, 0.053887173533439636, 0.030077779665589333, 0.06359510868787766]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings_model_huggingface = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    ")\n",
    "\n",
    "# Extract text content from Document objects\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# Embed your chunks\n",
    "print(f\"Embedding {len(chunk_texts)} chunks...\")\n",
    "chunk_embeddings = embeddings_model_huggingface.embed_documents(chunk_texts)\n",
    "\n",
    "print(f\"Created {len(chunk_embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(chunk_embeddings[0])}\")\n",
    "print(f\"First chunk: {chunk_texts[0][:100]}...\")\n",
    "print(f\"First embedding (first 5 values): {chunk_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7925bd",
   "metadata": {},
   "source": [
    "## 3 - Ingestion\n",
    "\n",
    "Store Text Embeddings In Vector Database with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b6183a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to MongoDB...\n",
      "✓ Connected to rag_database.document_chunks\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "# MongoDB Configuration\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\")\n",
    "db_name = os.getenv(\"MONGODB_DATABASE\")\n",
    "collection_name = os.getenv(\"MONGODB_COLLECTION\")\n",
    "\n",
    "print(\"\\nConnecting to MongoDB...\")\n",
    "client = MongoClient(mongodb_uri)\n",
    "client.admin.command('ping')\n",
    "database = client[db_name]\n",
    "collection = database[collection_name]\n",
    "print(f\"✓ Connected to {db_name}.{collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0c3dba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs after delete: 0\n"
     ]
    }
   ],
   "source": [
    "collection.delete_many({})\n",
    "print(\"Docs after delete:\", collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c5ad784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingesting 15 chunks into MongoDB...\n",
      "Docs after insert: 15\n"
     ]
    }
   ],
   "source": [
    "# Extract text content from Document objects\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "print(f\"\\nIngesting {len(chunk_texts)} chunks into MongoDB...\")\n",
    "vector_store = MongoDBAtlasVectorSearch.from_texts(\n",
    "    texts=chunk_texts,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=\"vector_index\"\n",
    ")\n",
    "print(\"Docs after insert:\", collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1d828",
   "metadata": {},
   "source": [
    "## 4 - Vector Search\n",
    "\n",
    "\n",
    "Vector search (also called semantic search or similarity search) is the process of finding the most relevant chunks of text from your database based on the meaning of a query, not just keyword matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a041879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current documents in collection: 15\n"
     ]
    }
   ],
   "source": [
    "# Check current count\n",
    "current_count = collection.count_documents({})\n",
    "print(f\"Current documents in collection: {current_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3a51e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Regularization, and what is the difference between L1 (Lasso) and L2 (Ridge)?\n",
      "\n",
      "Answer:\n",
      "Both Bagging and Boosting are ensemble techniques to improve model predictions, but \n",
      "they work differently. \n",
      " \n",
      "Comparison Table: \n",
      "1. Bagging: The simplest way of combining predictions that belong to the same type. \n",
      "Boosting: A way of combining predictions that belong to the different types.\n"
     ]
    }
   ],
   "source": [
    "def get_answer(query, vector_store, k=10):\n",
    "    results = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No results found\"\n",
    "    \n",
    "    # Use the top result\n",
    "    content = results[0].page_content\n",
    "    \n",
    "    # Extract answer between \"Ans:\" and next \"Q.\"\n",
    "    ans_start = content.find(\"Ans:\")\n",
    "    \n",
    "    if ans_start != -1:\n",
    "        # Find next question\n",
    "        next_q = content.find(\"Q.\", ans_start + 4)\n",
    "        \n",
    "        if next_q != -1:\n",
    "            answer = content[ans_start + 4:next_q].strip()\n",
    "        else:\n",
    "            answer = content[ans_start + 4:].strip()\n",
    "        \n",
    "        return answer\n",
    "    else:\n",
    "        return content.strip()\n",
    "\n",
    "# Test\n",
    "test_query = \"What is Regularization, and what is the difference between L1 (Lasso) and L2 (Ridge)?\"\n",
    "answer = get_answer(test_query, vector_store)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b4d4b",
   "metadata": {},
   "source": [
    "that is a bad answer:\n",
    "\n",
    "That usually happens for one of these reasons in a RAG pipeline:\n",
    "\n",
    "1- Retrieval is weak (the vector search returns irrelevant chunks)\n",
    "\n",
    "2- The LLM is answering without using the retrieved context\n",
    "\n",
    "3- Your chunks are too big / too small so the meaning is lost\n",
    "\n",
    "4- You’re not storing metadata (page number, source) so the model can’t ground answers\n",
    "\n",
    "5- Your prompt is not forcing “answer only from context”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a41908b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "INDEX_NAME = \"vector_index\"\n",
    "\n",
    "def ingest_pdf(pdf_path: str):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=120\n",
    "    )\n",
    "\n",
    "    docs = []\n",
    "    for page in pages:\n",
    "        chunks = splitter.split_text(page.page_content)\n",
    "        for chunk in chunks:\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page.metadata.get(\"page\", 0)\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "    collection.delete_many({})\n",
    "\n",
    "    vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection=collection,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ee979f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(vector_store, query: str, k: int = 6):\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    text = \"\"\n",
    "    for d in docs:\n",
    "        page = d.metadata.get(\"page\", \"N/A\")\n",
    "        source = d.metadata.get(\"source\", \"N/A\")\n",
    "        text += f\"[Source: {source} | Page: {page}]\\n{d.page_content}\\n\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e0209a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def ask_rag(vector_store, query: str):\n",
    "    docs = retrieve_docs(vector_store, query, k=6)\n",
    "\n",
    "    if not docs:\n",
    "        return \"No documents retrieved. Check MongoDB index or embeddings.\", []\n",
    "\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer ONLY using the context below.\n",
    "If the answer is not in the context, say:\n",
    "\"I don't know based on the document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    sources = []\n",
    "    for d in docs:\n",
    "        sources.append(\n",
    "            f\"{d.metadata.get('source')} (page {d.metadata.get('page')})\"\n",
    "        )\n",
    "\n",
    "    return answer, sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2d578186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(vector_store, query: str, k: int = 6):\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "\n",
    "    print(\"Retrieved inside ask_rag:\", len(docs))\n",
    "\n",
    "    if not docs:\n",
    "        return \"No documents retrieved. Check MongoDB index or embeddings.\", []\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"[Source: {d.metadata.get('source')} | Page: {d.metadata.get('page')}]\\n{d.page_content}\"\n",
    "            for d in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer ONLY using the context below.\n",
    "Write the answer in clear bullet points.\n",
    "If the answer is not in the context, say:\n",
    "\"I don't know based on the document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    sources = sorted(\n",
    "        set(\n",
    "            [f\"{d.metadata.get('source')} (page {d.metadata.get('page')})\" for d in docs]\n",
    "        ),\n",
    "        key=lambda x: int(x.split(\"page \")[-1].replace(\")\", \"\"))\n",
    "    )\n",
    "\n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47ae9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved inside ask_rag: 6\n",
      "Answer:\n",
      " - **AI Developers**: Create and develop AI models or systems, define intended use cases, and assess potential risks.\n",
      "- **AI Deployers**: Deploy AI systems to end users, assess suitability and performance in their unique operating context.\n",
      "- **AI End Users**: Provide inputs or receive outputs from an AI system and are encouraged to share feedback for improvements.\n",
      "\n",
      "Sources:\n",
      "- ../Data/MlQuestions_Final_Comprehensive.pdf (page 4)\n",
      "- ../Data/MlQuestions_Final_Comprehensive.pdf (page 5)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main AI roles defined by AWS (developers, deployers, end users)?\"\n",
    "answer, sources = ask_rag(vector_store, query)\n",
    "\n",
    "print(\"Answer:\\n\", answer)\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for s in sources:\n",
    "    print(\"-\", s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
